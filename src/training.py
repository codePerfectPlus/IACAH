# -*- coding: utf-8 -*-
"""training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/codePerfectPlus/IACAH/blob/main/src/training.ipynb

# IACAH (India Academia Connect AI Hackathon)

Date - October 4-13, 2021

Artificial intelligence will be an enormous part of the future workforce. Itâ€™s expected to generate 2 million net job gains versus losses by 2025. India Academia Connect AI Hackathon will help the participants from leading research institutions with the opportunity to learn and implement the latest AI technology, preparing them for a future AI-powered economy, with a large research and developer base.

Data Download Link:- [GDrive Link](https://drive.google.com/drive/folders/1O8TT0s4zMyiI6zR-biVRoiLiAUy-W1H0?usp=sharing)

#### Primary Goal:- To classify the Images into Background and Text
    To Classify the Images into two categories(Background or Text) using Tensorflow and Keras.

#### Solution:- 
    It's a Binary Class classification problem. It can be solved using CNN classifier.
    Train the Model using transfer learning. We used VGG16 as feature extractor and used Dropout to avoid overfitting. we used some ImageDataAugmentation technique to make model more robust and better. we are using modelcheckpoint and earlystopping callbacks function to get the best model possible

#### Result:- 
    we evalute the model on test_data(unseen data) using evlaute function and got the approx 92% accuracy. The Accuracy may be improve using new model like Resnet or Imagenet. Although Vgg16 is also works as feature extractor.
"""

import os
import json
import numpy as np
import pandas as pd
from PIL import Image
from matplotlib import pyplot as plt
import tensorflow as tf
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Model
from tensorflow.keras import layers
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import vgg16
from tensorflow.keras.applications.vgg16 import preprocess_input
from tensorflow.keras.losses import BinaryCrossentropy
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from tensorflow.keras.preprocessing import image

train_dir = "training" 
test_dir = "test"
batch_size = 50
img_size = (64, 64)
num_class = len(os.listdir(train_dir))
idx_to_name = os.listdir(train_dir)
name_to_idx = dict([(v, k) for k, v in enumerate(idx_to_name)])

def data_to_df(data_dir, subset=None):
    ''' Creating DataFrame for loading Filename and Label
    
    Args:
        Data_dir: 
            Data_dir Path
        Subset: 
            - train- for spliting data in train and val
    
     '''
    df = pd.DataFrame(columns=['filenames', 'labels'])

    filenames = []
    labels = []
    for dataset in os.listdir(data_dir):
        img_list = os.listdir(os.path.join(data_dir, dataset))

        label = name_to_idx[dataset]

        for image in img_list:
            filenames.append(os.path.join(data_dir, dataset, image))
            labels.append(label)

    df["filenames"] = filenames
    df["labels"] = labels
    
    if subset == "train":
        train_df, val_df = train_test_split(df, train_size=0.9, shuffle=True)    
        return train_df, val_df
    return df

train_df, val_df = data_to_df(train_dir, subset="train")
root_dir = os.path.dirname(os.path.abspath("training.py"))

class CustomDataGenerator:
    ''' Custom DataGenerator to load img '''
    def __init__(self, data_frame):
        self.data_frame = data_frame
    
    def load_samples(self):
        ''' Convert DataFrame into List '''
        data = self.data_frame
        data = data[["filenames", "labels"]]
        filenames = list(data["filenames"])
        labels = list(data["labels"])

        samples = []
        for filename, label in zip(filenames, labels):
            samples.append([filename, label])
        return samples
    
    def __len__(self):
        return len(self.data_frame)
    
    def preprocessing(self, img, label):
        img = np.resize(img, (64, 64, 3))
        img = preprocess_input(img)
        return img, label

    def generators(self, samples, batch_size=10, shuffle_data=True):
        ''' Generator function to yield img and laebl '''
        num_samples = len(samples)
        while True:
            samples = shuffle(samples)
            for offset in range(0, num_samples, batch_size):
                batch_samples = samples[offset:offset+batch_size]
                
                X_train = []
                y_train = []

                for batch_sample in batch_samples:
                    img_name = batch_sample[0]
                    label = batch_sample[1]

                    img = np.asarray(Image.open(os.path.join(root_dir, img_name)))
                    img, label = self.preprocessing(img, label)

                    X_train.append(img)
                    y_train.append(label)

                X_train = np.array(X_train)
                y_train = np.array(y_train)

                yield X_train, y_train

# creating customDataGenerator object
train_datagen = CustomDataGenerator(train_df)
val_datagen = CustomDataGenerator(val_df)

# creating sample with dataframe
train_sample = train_datagen.load_samples()
val_sample = val_datagen.load_samples()

# creating train and validation data
train_data = train_datagen.generators(train_sample, batch_size=batch_size)
val_data = val_datagen.generators(val_sample, batch_size=batch_size)

base_model = vgg16.VGG16(weights="imagenet", include_top=False, input_shape=(64, 64, 3))
base_model.trainable= True

inputs = layers.Input(shape=(64, 64, 3))
x = base_model(inputs)
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dense(128, activation="relu")(x)
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(1, activation="sigmoid")(x)

model = Model(inputs, outputs, name="VGG16")

model.summary()

model.compile(
    optimizer="adam",
    loss=BinaryCrossentropy(),
    metrics=["accuracy"])

modelcheckpoint = ModelCheckpoint(
    "tmp",
    save_best_only=True)

earlystopping = EarlyStopping(
    monitor="val_loss",
    patience=5,
    restore_best_weights=True)

callbacks = [earlystopping, modelcheckpoint]

try:
    model.load_weights("tmp")
    print("Loading weight from last checkpoint")
except Exception as e:
    print("no weight found. Training from scratch")

history = model.fit(
    train_data,
    epochs=15,
    batch_size=batch_size,
    steps_per_epoch=len(train_sample)//batch_size,
    validation_data=val_data,
    validation_steps=len(val_sample)//batch_size,
    callbacks=callbacks)

his = history.history

plt.plot(his["accuracy"])
plt.plot(his["val_accuracy"])
plt.legend(["Accuracy", "Val Accuracy"])
plt.show()

plt.plot(his["loss"])
plt.plot(his["val_loss"])
plt.legend(["Loss", "Val Loss"])
plt.show()

test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)

test_data = test_datagen.flow_from_directory(
    "test2",
    batch_size=batch_size,
    target_size=img_size,
    class_mode="binary")

loss, accuracy = model.evaluate(test_data)

print("Loss on Test Data", loss)
print("Accuracy on Test Data", accuracy)

# Creating Json File to submit the solution
final_output = {}
for folder in os.listdir("test2"):
    for image_name in os.listdir(os.path.join("test2", folder)):
        img_ = image.load_img(os.path.join("test2", folder, image_name), 
                              target_size=(64, 64), color_mode="rgb")
        img_arr = image.img_to_array(img_)
        img_arr = preprocess_input(img_arr)
        img_batch = np.array([img_arr])
        output = model.predict(img_batch)
        if output > 0.5:
            final_output[image_name] = 1
        else:
            final_output[image_name] = 0

with open("result.json", "w") as file:
    json.dump(final_output, file)

!zip -r tmp.zip tmp

from google.colab import files
files.download("tmp.zip")

